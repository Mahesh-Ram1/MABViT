JAX implementation of Modified attention. 

Paper: https://arxiv.org/pdf/2312.01324.pdf

How to use: Download Modiefied.attention.py and Call Modified_attention.MultiHeadDotProductAttention instead of flax.linen.MultiHeadDotProductAttention.
